name: Deploy to AWS Docker Swarm

on:
  push:
    branches:
      - main
      - production
  pull_request:
    branches:
      - main
      - production
  workflow_dispatch:

env:
  AWS_REGION: eu-north-1
  ECR_REGISTRY: ${{ secrets.ECR_REGISTRY }}
  SWARM_MANAGER_HOST: ${{ secrets.SWARM_MANAGER_HOST }}
  SWARM_MANAGER_USER: ubuntu

jobs:
  build-and-push:
    name: Build and Push Docker Images
    runs-on: ubuntu-latest
    if: github.event_name == 'push' || github.event_name == 'workflow_dispatch'

    outputs:
      image_tag: ${{ steps.meta.outputs.tags }}

    strategy:
      matrix:
        service: [backend]

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Ensure ECR Repository Exists
        run: |
          REPO_NAME=visionai-${{ matrix.service }}
          aws ecr describe-repositories --repository-names ${REPO_NAME} --region ${{ env.AWS_REGION }} || \
          aws ecr create-repository --repository-name ${REPO_NAME} --region ${{ env.AWS_REGION }}

      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ steps.login-ecr.outputs.registry }}/visionai-${{ matrix.service }}
          tags: |
            type=raw,value=latest,enable={{is_default_branch}}
            type=sha,prefix={{branch}}-

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build and push ${{ matrix.service }}
        uses: docker/build-push-action@v5
        with:
          context: ./${{ matrix.service == 'automation' && 'browser-automation' || matrix.service }}
          file: ./docker/${{ matrix.service }}.Dockerfile
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          platforms: linux/amd64

      - name: Image digest
        run: echo ${{ steps.meta.outputs.digest }}

  deploy:
    name: Deploy to Docker Swarm
    runs-on: ubuntu-latest
    needs: build-and-push
    if: github.ref == 'refs/heads/main' || github.ref == 'refs/heads/production'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Set up SSH key
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.SWARM_SSH_PRIVATE_KEY }}" > ~/.ssh/swarm_key
          chmod 600 ~/.ssh/swarm_key
          ssh-keyscan -H ${{ env.SWARM_MANAGER_HOST }} >> ~/.ssh/known_hosts

      - name: Set image tag
        id: set-tag
        run: |
          if [ "${{ github.ref }}" == "refs/heads/production" ]; then
            echo "IMAGE_TAG=production" >> $GITHUB_OUTPUT
          else
            echo "IMAGE_TAG=main-$(git rev-parse --short=7 HEAD)" >> $GITHUB_OUTPUT
          fi

      - name: Get ECR Login Password
        id: ecr-login
        run: |
          PASSWORD=$(aws ecr get-login-password --region ${{ env.AWS_REGION }})
          echo "::add-mask::$PASSWORD"
          echo "ECR_PASSWORD=$PASSWORD" >> $GITHUB_OUTPUT

      - name: Create .env.production file
        run: |
          cat <<'EOF' > .env.production
          # Project Information
          PROJECT_NAME=AI Job Application Platform
          PROJECT_VERSION=1.0.0

          # API Settings
          HOST=0.0.0.0
          PORT=8000
          DEBUG=false
          ENVIRONMENT=production
          BROWSER_AUTOMATION_URL=http://host.docker.internal:3001
          BROWSER_AUTOMATION_TOKEN=${BROWSER_AUTOMATION_TOKEN}
          HEADLESS=true

          # Security
          SECRET_KEY=${{ secrets.SECRET_KEY }}
          JWT_SECRET_KEY=${{ secrets.JWT_SECRET_KEY }}
          JWT_ALGORITHM=HS256
          ACCESS_TOKEN_EXPIRE_MINUTES=30
          REFRESH_TOKEN_EXPIRE_MINUTES=10080
          PASSWORD_RESET_TOKEN_EXPIRE_HOURS=24

          # CORS
          CORS_ORIGINS=["https://synovae.io"]

          # AWS
          AWS_ACESS_KEY_SECRET=${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_ACCESS_ID=${{ secrets.AWS_ACCESS_KEY_ID }}

          # Redis
          REDIS_URL=redis://redis:6379/0
          REDIS_HOST=redis
          REDIS_PORT=6379
          REDIS_DB=0
          CACHE_TTL=3600

          # OpenAI
          OPENAI_API_KEY=${{ secrets.OPENAI_API_KEY }}
          OPENAI_MODEL=gpt-4
          OPENAI_MAX_TOKENS=2000
          OPENAI_TEMPERATURE=0.7

          # paystack
          PAYSTACK_PUBLIC_KEY=${{ secrets.PAYSTACK_PUBLIC_KEY }}
          PAYSTACK_SECRET_KEY=${{ secrets.PAYSTACK_SECRET_KEY }}

          # Stripe
          STRIPE_SECRET_KEY=${{ secrets.STRIPE_API_KEY }}
          STRIPE_PUBLISHABLE_KEY=${{ secrets.STRIPE_PUBLISHABLE_KEY }}
          STRIPE_WEBHOOK_SECRET=${{ secrets.STRIPE_WEBHOOK_SECRET }}
          STRIPE_BASIC_PRICE_ID=${{ secrets.STRIPE_BASIC_PRICE_ID }}
          STRIPE_PREMIUM_PRICE_ID=${{ secrets.STRIPE_PREMIUM_PRICE_ID }}


          MAIL_USERNAME=${{ secrets.MAIL_USERNAME }}
          MAIL_PASSWORD=${{ secrets.MAIL_PASSWORD }}
          MAIL_FROM=noreply@synovae.io
          MAIL_FROM_NAME=Synovae
          MAIL_PORT=587
          MAIL_SERVER=smtp.privateemail.com
          MAIL_STARTTLS=true
          MAIL_SSL_TLS=false
          SUPPORT_EMAIL=support@synovae.io
          COMPANY_NAME=Synovae

          # OAuth
          GOOGLE_CLIENT_ID=${{ secrets.GOOGLE_CLIENT_ID }}
          GOOGLE_CLIENT_SECRET=${{ secrets.GOOGLE_CLIENT_SECRET }}
          GOOGLE_REDIRECT_URI=https://synovae.io/api/v1/auth/google/callback
          LINKEDIN_CLIENT_ID=${{ secrets.LINKEDIN_CLIENT_ID }}
          LINKEDIN_CLIENT_SECRET=${{ secrets.LINKEDIN_CLIENT_SECRET }}
          LINKEDIN_REDIRECT_URI=https://synovae.io/auth/linkedin/callback

          # Frontend
          FRONTEND_URL=https://synovae.io

          # Job Boards
          INDEED_PUBLISHER_ID=${{ secrets.INDEED_PUBLISHER_ID }}
          INDEED_API_KEY=${{ secrets.INDEED_API_KEY }}

          # File Upload
          UPLOAD_DIR=uploads
          MAX_FILE_SIZE=10485760
          MAX_UPLOAD_SIZE=10485760

          # Rate Limiting
          RATE_LIMIT_REQUESTS=100
          RATE_LIMIT_PERIOD=60

          LINKEDIN_CLIENT_ID=${{ secrets.LINKEDIN_CLIENT_ID }}
          LINKEDIN_CLIENT_SECRET=${{ secrets.LINKEDIN_CLIENT_SECRET }}
          LINKEDIN_REDIRECT_URI=${{ secrets.LINKEDIN_REDIRECT_URI }}

          MONGODB_URL=${{ secrets.MONGODB_URL }}
          DATABASE_NAME=${{ secrets.DATABASE_NAME }}

          # Subscription Limits - Free
          FREE_TIER_MONTHLY_ATTEMPTS=3
          FREE_TIER_JOBS_PER_ATTEMPT=10

          # Subscription Limits - Basic
          BASIC_TIER_DAILY_ATTEMPTS=5
          BASIC_TIER_MONTHLY_ATTEMPTS=100
          BASIC_TIER_JOBS_PER_ATTEMPT=50
          BASIC_TIER_PRICE=999

          # Subscription Limits - Premium
          PREMIUM_TIER_MONTHLY_ATTEMPTS=999999
          PREMIUM_TIER_JOBS_PER_ATTEMPT=200
          PREMIUM_TIER_PRICE=2999

          # Referral System
          REFERRALS_FOR_FREE_ATTEMPT=3
          BASIC_REFERRAL_BONUS=10
          PREMIUM_REFERRAL_BONUS=20

          # Machine Learning
          ML_MODEL_PATH=models
          ML_TRAINING_DATA_PATH=data/training
          ML_RETRAIN_INTERVAL_DAYS=30

          # Scraping
          SCRAPER_USER_AGENT=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36
          SCRAPER_DELAY_MIN=1
          SCRAPER_DELAY_MAX=3

          # Logging
          LOG_LEVEL=INFO
          LOG_FILE=logs/app.log

          # Celery
          CELERY_BROKER_URL=redis://redis:6379/0
          CELERY_RESULT_BACKEND=redis://redis:6379/0

          # Webhooks
          WEBHOOK_SECRET=${{ secrets.WEBHOOK_SECRET }}

          # ECR (for deployment)
          ECR_PASSWORD=${{ steps.ecr-login.outputs.ECR_PASSWORD }}
          EOF

      - name: Deploy to Swarm
        env:
          IMAGE_TAG: ${{ steps.set-tag.outputs.IMAGE_TAG }}
        run: |
          # Copy files to manager
          # Copy configuration files
          scp -o StrictHostKeyChecking=no -i ~/.ssh/swarm_key \
            docker-compose.swarm.yml .env.production \
            ${{ env.SWARM_MANAGER_USER }}@${{ env.SWARM_MANAGER_HOST }}:/tmp/

          # Copy frontend files
          scp -r -o StrictHostKeyChecking=no -i ~/.ssh/swarm_key \
            frontend \
            ${{ env.SWARM_MANAGER_USER }}@${{ env.SWARM_MANAGER_HOST }}:/tmp/
            
          # Copy automation files
          scp -r -o StrictHostKeyChecking=no -i ~/.ssh/swarm_key \
            browser-automation \
            ${{ env.SWARM_MANAGER_USER }}@${{ env.SWARM_MANAGER_HOST }}:/tmp/

          # Move frontend files to /var/www/visionai
          ssh -o StrictHostKeyChecking=no -i ~/.ssh/swarm_key \
            ${{ env.SWARM_MANAGER_USER }}@${{ env.SWARM_MANAGER_HOST }} << 'ENDSSH'
            
            echo "Deploying frontend files..."
            sudo mkdir -p /var/www/visionai
            # Create backup if exists
            # sudo cp -r /var/www/visionai /var/www/visionai_backup_$(date +%s) || true
            
            # Copy new files
            sudo cp -r /tmp/frontend/* /var/www/visionai/
            
            # Set permissions (nginx needs read access)
            sudo chown -R www-data:www-data /var/www/visionai
            sudo chmod -R 755 /var/www/visionai
            
            # Set permissions (nginx needs read access)
            sudo chown -R www-data:www-data /var/www/visionai
            sudo chmod -R 755 /var/www/visionai
            
            # Allow port 3001 for automation service
            sudo ufw allow 3001/tcp || true
            
            # Cleanup any old processes on port 3000 (just in case)
            sudo fuser -k 3000/tcp || true
            sudo fuser -k 3001/tcp || true
            
            # Cleanup tmp
            rm -rf /tmp/frontend
            
            echo "Frontend deployed successfully."
            
            # --- Deploy Browser Automation ---
            echo "Deploying browser automation..."
            # Using home directory instead of var/www to separate from frontend
            TARGET_DIR="/home/ubuntu/visionai-automation"
            mkdir -p $TARGET_DIR
            
            # Copy new files
            cp -r /tmp/browser-automation/* $TARGET_DIR/
            
            # Install dependencies and start with PM2
            cd $TARGET_DIR
            
            # Ensure Node.js and NPM are installed
            if ! command -v npm &> /dev/null; then
                echo "Installing Node.js and NPM..."
                curl -fsSL https://deb.nodesource.com/setup_20.x | sudo -E bash -
                sudo apt-get install -y nodejs
            fi

            # Ensure PM2 is installed
            if ! command -v pm2 &> /dev/null; then
                echo "Installing pm2..."
                sudo npm install -g pm2
            fi
            
            echo "Installing dependencies..."
            npm install --production
            
            echo "Starting/Restarting service..."
            # Using --update-env to pick up the token from the environment variable
            # We explicitly set PORT and BROWSER_AUTOMATION_TOKEN.
            # We also ensure the old named process is deleted first for a clean start
            pm2 delete "visionai-automation" || true
            PORT=3001 HEADLESS=true BROWSER_AUTOMATION_TOKEN="${{ secrets.BROWSER_AUTOMATION_TOKEN }}" pm2 start src/index.js --name "visionai-automation" --update-env
            
            pm2 save
            
            # Wait for service to start
            sleep 5
            
            # Health Check
            echo "Checking automation service health..."
            curl -v http://localhost:3001/health || echo "⚠️ Warning: Automation service check failed locally"
            
            pm2 save
            
            # Cleanup tmp
            rm -rf /tmp/browser-automation
            
            echo "Browser automation deployed successfully."
          ENDSSH

          # Deploy stack
          ssh -o StrictHostKeyChecking=no -i ~/.ssh/swarm_key \
            ${{ env.SWARM_MANAGER_USER }}@${{ env.SWARM_MANAGER_HOST }} << 'ENDSSH'
            
            # Helper function to create secret from env file
            create_secret_from_env() {
              local name=$1
              local value=$2
              
              # Skip if value is empty
              if [ -z "$value" ]; then
                echo "Skipping empty secret: $name"
                return
              fi
              
              # Check if secret exists and remove it to force update
              if docker secret inspect "$name" > /dev/null 2>&1; then
                echo "Removing existing secret to force update: $name"
                docker secret rm "$name"
              fi
              
              echo "Creating secret: $name"
              echo -n "$value" | docker secret create "$name" -
            }

            # Read .env.production and create secrets
            echo "Creating Docker secrets from .env.production..."
            
            # Parse .env.production safely (instead of sourcing it)
            while IFS='=' read -r key value || [ -n "$key" ]; do
              # Skip comments and empty lines
              [[ "$key" =~ ^#.*$ ]] || [[ -z "$key" ]] && continue
              
              # Remove leading/trailing whitespace and quotes
              key=$(echo "$key" | xargs)
              value=$(echo "$value" | xargs)
              value="${value%\"}"
              value="${value#\"}"
              value="${value%\'}"
              value="${value#\'}"
              
              # Export the variable
              export "$key=$value"
            done < /tmp/.env.production
            
            # Remove legacy lowercase secrets to prevent conflicts
            docker secret rm openai_api_key 2>/dev/null || true
            docker secret rm stripe_api_key 2>/dev/null || true
            
            # Create secrets for sensitive values only
            create_secret_from_env "OPENAI_API_KEY" "${OPENAI_API_KEY}"
            create_secret_from_env "STRIPE_SECRET_KEY" "${STRIPE_SECRET_KEY}"
            create_secret_from_env "PAYSTACK_PUBLIC_KEY" "${PAYSTACK_PUBLIC_KEY}"
            create_secret_from_env "PAYSTACK_SECRET_KEY" "${PAYSTACK_SECRET_KEY}"
            create_secret_from_env "JWT_SECRET_KEY" "${JWT_SECRET_KEY}"
            create_secret_from_env "GOOGLE_CLIENT_ID" "${GOOGLE_CLIENT_ID}"
            create_secret_from_env "LINKEDIN_CLIENT_ID" "${LINKEDIN_CLIENT_ID}"
            create_secret_from_env "LINKEDIN_CLIENT_SECRET" "${LINKEDIN_CLIENT_SECRET}"
            create_secret_from_env "LINKEDIN_REDIRECT_URI" "${LINKEDIN_REDIRECT_URI}"
            create_secret_from_env "MONGODB_URL" "${MONGODB_URL}"
            create_secret_from_env "DATABASE_NAME" "${DATABASE_NAME}"
            create_secret_from_env "BROWSER_AUTOMATION_TOKEN" "${BROWSER_AUTOMATION_TOKEN}"
            create_secret_from_env "BROWSER_AUTOMATION_URL" "${BROWSER_AUTOMATION_URL}"
            
            # Check disk space before deploy
            echo "=== Disk Space ==="
            df -h
            docker system df

            # Verify secrets exist
            echo "=== Secrets Verification ==="
            docker secret ls

            # Set environment variables for stack deploy
            export ECR_REGISTRY=${{ steps.login-ecr.outputs.registry }}
            export IMAGE_TAG=${{ steps.set-tag.outputs.IMAGE_TAG }}
            
            # Login to ECR
            aws ecr get-login-password --region ${{ env.AWS_REGION }} | docker login --username AWS --password-stdin ${{ steps.login-ecr.outputs.registry }}
            
            # Pull backend image
            echo "Pulling backend image..."
            docker pull ${ECR_REGISTRY}/visionai-backend:${IMAGE_TAG}
            
            # Deploy stack with rolling update
            docker stack deploy \
              --compose-file /tmp/docker-compose.swarm.yml \
              --with-registry-auth \
              visionai
            
            # Wait for services to be updated
            echo "Waiting for services to update..."
            sleep 60
            
            # Check service status
            docker service ls
            
            # Count total expected services
            EXPECTED_COUNT=5
            ACTUAL_COUNT=$(docker service ls --filter "label=com.docker.stack.namespace=visionai" -q | wc -l)
            
            if [ "$ACTUAL_COUNT" -lt "$EXPECTED_COUNT" ]; then
               echo "⚠️  Warning: Stack incomplete! Expected $EXPECTED_COUNT services, found $ACTUAL_COUNT."
               echo "=== Stack Tasks (All) ==="
               docker stack ps visionai --no-trunc
            fi

            FAILED=$(docker service ls --filter "label=com.docker.stack.namespace=visionai" --format "{{.Replicas}}" | grep "0/")
            if [ ! -z "$FAILED" ]; then
              echo "⚠️  Warning: Some services failed to start!"
              docker service ls
              echo "=== Stack Tasks ==="
              docker stack ps visionai --no-trunc
              echo "=== Backend Logs ==="
              docker service logs --tail 50 visionai_backend || true
              echo "=== Celery Beat Logs ==="
              docker service logs --tail 50 visionai_celery-beat || true
            fi
            
            echo "Deployment completed!"
            
            # Cleanup env file (contains secrets)
            rm -f /tmp/.env.production
          ENDSSH

      - name: Ensure Admin User
        run: |
          ssh -o StrictHostKeyChecking=no -i ~/.ssh/swarm_key \
            ${{ env.SWARM_MANAGER_USER }}@${{ env.SWARM_MANAGER_HOST }} << 'ENDSSH'
            
            echo "=== Creating Admin User ==="
            # Wait a moment for services to stabilize if needed
            sleep 10
            
            # Get one backend container ID
            CONTAINER_ID=$(docker ps --filter "name=visionai_backend" -q | head -n 1)
            
            if [ -z "$CONTAINER_ID" ]; then
              echo "⚠️  Warning: Backend container not found running on manager node."
              echo "Note: In Swarm, the container might be on a worker node. Skipping admin creation for this run."
              # In a robust setup, we might need `docker service ps` to find where it is or use a one-off service.
              # However, for this single-node/small setup, it's likely on the manager or accessible.
              # If we truly need to run it regardless of node:
              # docker service create --restart-condition=none --name admin_creator --network visionai_default ...
              # For now, we'll try the direct exec approach assuming single node or manager placement.
            else
              echo "Found backend container: $CONTAINER_ID"
              echo "Running create_admin.py..."
              docker exec $CONTAINER_ID python scripts/create_admin.py
            fi
          ENDSSH

      - name: Verify deployment
        continue-on-error: true
        run: |
          ssh -i ~/.ssh/swarm_key \
            ${{ env.SWARM_MANAGER_USER }}@${{ env.SWARM_MANAGER_HOST }} << 'ENDSSH'
            
            echo "=== Stack Tasks ==="
            docker stack ps visionai --no-trunc

            echo "=== Node Status ==="
            docker node ls
            
            echo "=== Disk Space ==="
            df -h
            
            echo "=== Recent Backend Logs ==="
            docker service logs --tail 50 visionai_backend || true
            
            echo "=== Recent Celery Beat Logs ==="
            docker service logs --tail 50 visionai_celery-beat || true
            
            echo "=== Internal Health Check ==="
            curl -v -k --connect-timeout 10 https://127.0.0.1/api/v1/health || echo "Internal Check Failed"

            echo "=== External Health Check ==="
            sleep 10
            curl -f --connect-timeout 10 https://synovae.io/api/v1/health || echo "External check failed, but continuing..."
            
            echo "Verification complete!"
          ENDSSH

      - name: Cleanup
        if: always()
        run: |
          rm -f ~/.ssh/swarm_key
          rm -f .env.production

  notify:
    name: Send Deployment Notification
    runs-on: ubuntu-latest
    needs: deploy
    if: always()

    steps:
      - name: Send notification
        run: |
          if [ "${{ needs.deploy.result }}" == "success" ]; then
            echo "✅ Deployment to synovae.io completed!"
          else
            echo "⚠️  Deployment completed with warnings - check logs!"
          fi
